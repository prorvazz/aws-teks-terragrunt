{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"tEKS \u00b6 tEKS is a set of Terraform / Terragrunt modules designed to get you everything you need to run a production EKS cluster on AWS. It ships with sensible defaults, and add a lot of common addons with their configurations that work out of the box. the v5 and further version of this project have been completely revamp and now offer a skeleton to use as a base for your infrastructure projects around EKS. All the modules have been moved outside this repository and get their own versioning. The old README is accessible here Terraform/Terragrunt \u00b6 Terraform implementation is available in the terraform folder. Terragrunt implementation is available in the terragrunt folder. Requirements \u00b6 Terraform \u00b6 Terraform direnv : available in every Linux distribution tfenv Terragrunt \u00b6 Terraform Terragrunt Main purposes \u00b6 The main goal of this project is to glue together commonly used tooling with Kubernetes/EKS and to get from an AWS Account to a production cluster with everything you need without any manual configuration. What you get \u00b6 A production cluster all defined in IaaC with Terraform/Terragrunt: AWS VPC if needed based on terraform-aws-vpc EKS cluster base on terraform-aws-eks Kubernetes addons based on terraform-kubernetes-addons : provides various addons that are often used on Kubernetes and specifically on EKS. Kubernetes namespaces quota management based on terraform-kubernetes-namespaces : allows administrator to manage namespaces and quotas from a centralized configuration with Terraform. Everything is tied together with Terragrunt and allows you to deploy a multi cluster architecture in a matter of minutes (ok maybe an hour) and different AWS accounts for different environments. Curated Features \u00b6 The main additionals features are the curated addons list, see here and in the customization of the cluster policy Enforced security \u00b6 Default PSP is removed and sensible defaults are enforced All addons have specific PSP enabled No IAM credentials on instances, everything is enforced with IRSA or KIAM Each addons is deployed in it's own namespace with sensible default network policies Out of the box monitoring \u00b6 Prometheus Operator with defaults dashboards Addons that support metrics are enable along with their serviceMonitor Custom grafana dashboard are available by default. Helm v3 provider \u00b6 All addons support Helm v3 configuration All charts are easily customizable Other and not limited to \u00b6 priorityClasses for addons use of [ kubectl-provider ], no more local exec and custom manifest are properly handled lot of manual stuff have been automated under the hood Requirements \u00b6 Terragrunt is not a hard requirement but all the modules are tested with Terragrunt. Terraform Terragrunt kubectl helm aws-iam-authenticator Examples \u00b6 terraform/live folder provides an opinionated directory structure for a production environment with an example using Additional infrastructure blocks \u00b6 If you wish to extend your infrastructure you can pick up additional modules on the particuleio github page . Some modules can also be found on the clusterfrak-dynamics github page . Branches \u00b6 main : Backward incompatible with v1.X but compatible with v2.X, releases bumped to v3.X because a lot has changed. release-1.X : Compatible with Terraform < 0.12 and Terragrunt < 0.19. Be sure to target the same modules version. release-2.X : Compatible with Terraform >= 0.12 and Terragrunt >= 0.19. Be sure to target the same modules version. License \u00b6","title":"Overview"},{"location":"#teks","text":"tEKS is a set of Terraform / Terragrunt modules designed to get you everything you need to run a production EKS cluster on AWS. It ships with sensible defaults, and add a lot of common addons with their configurations that work out of the box. the v5 and further version of this project have been completely revamp and now offer a skeleton to use as a base for your infrastructure projects around EKS. All the modules have been moved outside this repository and get their own versioning. The old README is accessible here","title":"tEKS"},{"location":"#terraformterragrunt","text":"Terraform implementation is available in the terraform folder. Terragrunt implementation is available in the terragrunt folder.","title":"Terraform/Terragrunt"},{"location":"#requirements","text":"","title":"Requirements"},{"location":"#terraform","text":"Terraform direnv : available in every Linux distribution tfenv","title":"Terraform"},{"location":"#terragrunt","text":"Terraform Terragrunt","title":"Terragrunt"},{"location":"#main-purposes","text":"The main goal of this project is to glue together commonly used tooling with Kubernetes/EKS and to get from an AWS Account to a production cluster with everything you need without any manual configuration.","title":"Main purposes"},{"location":"#what-you-get","text":"A production cluster all defined in IaaC with Terraform/Terragrunt: AWS VPC if needed based on terraform-aws-vpc EKS cluster base on terraform-aws-eks Kubernetes addons based on terraform-kubernetes-addons : provides various addons that are often used on Kubernetes and specifically on EKS. Kubernetes namespaces quota management based on terraform-kubernetes-namespaces : allows administrator to manage namespaces and quotas from a centralized configuration with Terraform. Everything is tied together with Terragrunt and allows you to deploy a multi cluster architecture in a matter of minutes (ok maybe an hour) and different AWS accounts for different environments.","title":"What you get"},{"location":"#curated-features","text":"The main additionals features are the curated addons list, see here and in the customization of the cluster policy","title":"Curated Features"},{"location":"#enforced-security","text":"Default PSP is removed and sensible defaults are enforced All addons have specific PSP enabled No IAM credentials on instances, everything is enforced with IRSA or KIAM Each addons is deployed in it's own namespace with sensible default network policies","title":"Enforced security"},{"location":"#out-of-the-box-monitoring","text":"Prometheus Operator with defaults dashboards Addons that support metrics are enable along with their serviceMonitor Custom grafana dashboard are available by default.","title":"Out of the box monitoring"},{"location":"#helm-v3-provider","text":"All addons support Helm v3 configuration All charts are easily customizable","title":"Helm v3 provider"},{"location":"#other-and-not-limited-to","text":"priorityClasses for addons use of [ kubectl-provider ], no more local exec and custom manifest are properly handled lot of manual stuff have been automated under the hood","title":"Other and not limited to"},{"location":"#requirements_1","text":"Terragrunt is not a hard requirement but all the modules are tested with Terragrunt. Terraform Terragrunt kubectl helm aws-iam-authenticator","title":"Requirements"},{"location":"#examples","text":"terraform/live folder provides an opinionated directory structure for a production environment with an example using","title":"Examples"},{"location":"#additional-infrastructure-blocks","text":"If you wish to extend your infrastructure you can pick up additional modules on the particuleio github page . Some modules can also be found on the clusterfrak-dynamics github page .","title":"Additional infrastructure blocks"},{"location":"#branches","text":"main : Backward incompatible with v1.X but compatible with v2.X, releases bumped to v3.X because a lot has changed. release-1.X : Compatible with Terraform < 0.12 and Terragrunt < 0.19. Be sure to target the same modules version. release-2.X : Compatible with Terraform >= 0.12 and Terragrunt >= 0.19. Be sure to target the same modules version.","title":"Branches"},{"location":"#license","text":"","title":"License"},{"location":"user-guides/eks-addons/","text":"EKS addons module \u00b6 terraform-kubernetes-addons:aws is a custom module maintained here and provides: helm v3 charts manifests operators For commonly used addons one Kubernetes and most specifically with EKS. The deployments are curated to be tightly integrated with AWS and EKS. The following addons are available and work out of the box. Helm charts \u00b6 All charts have been tested with Helm v3 and the terraform-provider-helm v1.0 which supports Helm v3. They can be easily customize with custom values. cluster-autoscaler : scale worker nodes based on workload. external-dns : sync ingress and service records in route53. cert-manager : automatically generate TLS certificates, supports ACME v2. nginx-ingress : processes Ingress object and acts as a HTTP/HTTPS proxy (compatible with cert-manager). metrics-server : enable metrics API and horizontal pod scaling (HPA). prometheus-operator : Monitoring / Alerting / Dashboards. fluentd-cloudwatch : forwards logs to AWS Cloudwatch. node-problem-detector : Forwards node problems to Kubernetes events flux : Continuous Delivery with Gitops workflow. sealed-secrets : Technology agnostic, store secrets on git. kong : API Gateway ingress controller. keycloak : Identity and access management Kubernetes Manifests \u00b6 Kubernetes manifests are deployed with terraform-provider-kubectl cni-metrics-helper : Provides cloudwatch metrics for VPC CNI plugins. Operator \u00b6 Some project are transitioning to Operators . Istio is going to drop Helm support and is not compatible with Helm v3 so it has been removed and replaced with the Istio operator istio-operator : Service mesh for Kubernetes. IAM permissions \u00b6 Some addons require specific IAM permission. This can be done by either: IRSA: IAM role for service account which is the default and recommended way Addons that need IAM access have two variables: create_resources_irsa : default to true and uses IAM role for service account There is no specific config, everything is taken care of by the module. Customization \u00b6 All the configuration is done in eks-addons/terragrunt.hcl . i n clude { pa t h = \"${find_in_parent_folders()}\" } terraf orm { source = \"github.com/particuleio/terraform-kubernetes-addons.git//modules/aws?ref=main\" } depe n de n cy \"eks\" { co nf ig_pa t h = \"../eks\" mock_ou t pu ts = { clus ter _id = \"cluster-name\" clus ter _oidc_issuer_url = \"https://oidc.eks.eu-west-3.amazonaws.com/id/0000000000000000\" } } depe n de n cy \"vpc\" { co nf ig_pa t h = \"../vpc\" mock_ou t pu ts = { priva te _sub nets _cidr_blocks = [ \"10.0.0.0/16\" , \"192.168.0.0/24\" ] } } ge nerate \"provider\" { pa t h = \"provider.tf\" i f _exis ts = \"overwrite\" co ntents = << -E OF provider \"aws\" { regio n = \"${local.aws_region}\" } provider \"kubectl\" { hos t = da ta .aws_eks_clus ter .clus ter .e n dpoi nt clus ter _ca_cer t i f ica te = base 64 decode(da ta .aws_eks_clus ter .clus ter .cer t i f ica te _au t hori t y. 0. da ta ) t oke n = da ta .aws_eks_clus ter _au t h.clus ter . t oke n load_co nf ig_ f ile = false } provider \"kubernetes\" { hos t = da ta .aws_eks_clus ter .clus ter .e n dpoi nt clus ter _ca_cer t i f ica te = base 64 decode(da ta .aws_eks_clus ter .clus ter .cer t i f ica te _au t hori t y. 0. da ta ) t oke n = da ta .aws_eks_clus ter _au t h.clus ter . t oke n } provider \"helm\" { kuber netes { hos t = da ta .aws_eks_clus ter .clus ter .e n dpoi nt clus ter _ca_cer t i f ica te = base 64 decode(da ta .aws_eks_clus ter .clus ter .cer t i f ica te _au t hori t y. 0. da ta ) t oke n = da ta .aws_eks_clus ter _au t h.clus ter . t oke n } } da ta \"aws_eks_cluster\" \"cluster\" { na me = var.clus ter - na me } da ta \"aws_eks_cluster_auth\" \"cluster\" { na me = var.clus ter - na me } EOF } locals { aws_regio n = yamldecode( f ile( \"${find_in_parent_folders(\" regio n _values.yaml \")}\" )) [ \"aws_region\" ] cus t om_ ta gs = merge( yamldecode( f ile( \"${find_in_parent_folders(\" global_ ta gs.yaml \")}\" )) , yamldecode( f ile( \"${find_in_parent_folders(\" e n v_ ta gs.yaml \")}\" )) ) de fault _domai n _ na me = yamldecode( f ile( \"${find_in_parent_folders(\" global_values.yaml \")}\" )) [ \"default_domain_name\" ] de fault _domai n _su ff ix = \"${local.custom_tags[\" E n v \"]}.${local.custom_tags[\" Projec t \"]}.${local.default_domain_name}\" } i n pu ts = { clus ter - na me = depe n de n cy.eks.ou t pu ts .clus ter _id ta gs = merge( local.cus t om_ ta gs ) eks = { \"cluster_oidc_issuer_url\" = depe n de n cy.eks.ou t pu ts .clus ter _oidc_issuer_url } aws -e bs - csi - driver = { e na bled = true is_de fault _class = true } aws - f or - fluent - bi t = { e na bled = true } aws - load - bala n cer - co ntr oller = { e na bled = true } aws - n ode - ter mi nat io n - ha n dler = { e na bled = true } calico = { e na bled = true } cer t - ma na ger = { e na bled = true acme_email = \"cert@particule.io\" acme_h tt p 01 _e na bled = true acme_h tt p 01 _i n gress_class = \"nginx\" acme_d ns 01 _e na bled = true allowed_cidrs = depe n de n cy.vpc.ou t pu ts .priva te _sub nets _cidr_blocks experime ntal _csi_driver = true } clus ter - au t oscaler = { e na bled = true } c n i - me tr ics - helper = { e na bled = true } ex ternal - d ns = { ex ternal - d ns = { e na bled = true }, } i n gress - n gi n x = { e na bled = true use_ nl b_ip = true allowed_cidrs = depe n de n cy.vpc.ou t pu ts .priva te _sub nets _cidr_blocks } is t io - opera t or = { e na bled = true } karma = { e na bled = true ex tra _values = << -E XTRA_VALUES i n gress : e na bled : true pa t h : / a nn o tat io ns : kuber netes .io/i n gress.class : n gi n x cer t - ma na ger.io/clus ter - issuer : \"letsencrypt\" hos ts : - karma.$ { local.de fault _domai n _su ff ix } tls : - secre t Name : karma.$ { local.de fault _domai n _su ff ix } hos ts : - karma.$ { local.de fault _domai n _su ff ix } e n v : - na me : ALERTMANAGER_URI value : \"http://kube-prometheus-stack-alertmanager.monitoring.svc.cluster.local:9093\" - na me : ALERTMANAGER_PROXY value : \"true\" - na me : FILTERS_DEFAULT value : \"@state=active severity!=info severity!=none\" EXTRA_VALUES } keycloak = { e na bled = true } ko n g = { e na bled = true } kube - prome t heus - s ta ck = { e na bled = true allowed_cidrs = depe n de n cy.vpc.ou t pu ts .priva te _sub nets _cidr_blocks t ha n os_sidecar_e na bled = true t ha n os_bucke t _ f orce_des tr oy = true ex tra _values = << -E XTRA_VALUES gra fana : deployme nt S trate gy : t ype : Recrea te i n gress : e na bled : true a nn o tat io ns : kuber netes .io/i n gress.class : n gi n x cer t - ma na ger.io/clus ter - issuer : \"letsencrypt\" hos ts : - gra fana .$ { local.de fault _domai n _su ff ix } tls : - secre t Name : gra fana .$ { local.de fault _domai n _su ff ix } hos ts : - gra fana .$ { local.de fault _domai n _su ff ix } persis ten ce : e na bled : true s t orageClassName : ebs - sc accessModes : - ReadWri te O n ce size : 1 Gi prome t heus : prome t heusSpec : replicas : 1 re tent io n : 2 d re tent io n Size : \"6GB\" ruleSelec t orNilUsesHelmValues : false serviceMo n i t orSelec t orNilUsesHelmValues : false podMo n i t orSelec t orNilUsesHelmValues : false s t orageSpec : volumeClaimTempla te : spec : s t orageClassName : ebs - sc accessModes : [ \"ReadWriteOnce\" ] resources : reques ts : s t orage : 10 Gi EXTRA_VALUES } loki - s ta ck = { e na bled = true bucke t _ f orce_des tr oy = true } me tr ics - server = { e na bled = true allowed_cidrs = depe n de n cy.vpc.ou t pu ts .priva te _sub nets _cidr_blocks } n pd = { e na bled = true } sealed - secre ts = { e na bled = true } t ha n os = { e na bled = true bucke t _ f orce_des tr oy = true } } Default charts values \u00b6 Some values are defined by default directly into the module. These can off course be overridden and or merged/replaced. You can find the defaults values in the upstream module . Eg. default values for cluster-autoscaler are in cluster-autoscaler.tf . Overriding Helm provider values \u00b6 Helm provider have defaults values defined here : helm_de faults _de faults = { a t omic = false clea nu p_o n _ fa il = false depe n de n cy_upda te = false disable_crd_hooks = false disable_webhooks = false f orce_upda te = false recrea te _pods = false re n der_subchar t _ n o tes = true replace = false rese t _values = false reuse_values = false skip_crds = false t imeou t = 3600 veri f y = false wai t = true ex tra _values = \"\" } These can be overridden globally with the helm_defaults input variable or can be overridden per chart in terragrunt.hcl : helm_de faults = { replace = true veri f y = true t imeou t = 300 } clus ter _au t oscaler = { crea te _iam_resources_irsa = true iam_policy_override = \"\" versio n = \"v1.14.7\" char t _versio n = \"6.4.0\" e na bled = true de fault _ net work_policy = true clus ter _ na me = depe n de n cy.eks.ou t pu ts .clus ter _id t imeou t = 3600 <= here you ca n add a n y helm provider override } Overriding charts values.yaml \u00b6 It is possible to add or override values per charts. Helm provider use the same merge logic as Helm so you can basically rewrite the whole values.yaml if needed. Each chart has a extra_values variable where you can specify custom values. flu x = { crea te _iam_resources_irsa = true versio n = \"1.18.0\" char t _versio n = \"1.2.0\" e na bled = false de fault _ net work_policy = true ex tra _values = <<EXTRA_VALUES gi t : url : \"ssh://git@gitlab.com/myrepo/gitops-${local.env}.git\" pollI nter val : \"2m\" rbac : crea te : false regis tr y : au t oma t io n I nter val : \"2m\" EXTRA_VALUES } There are some examples in the terragrunt.hcl file. Not all the variables available are present. If you want a full list of variable, you can find them in the upstream module . For example for cluster-autoscaler you can see the default here .","title":"EKS Addons"},{"location":"user-guides/eks-addons/#eks-addons-module","text":"terraform-kubernetes-addons:aws is a custom module maintained here and provides: helm v3 charts manifests operators For commonly used addons one Kubernetes and most specifically with EKS. The deployments are curated to be tightly integrated with AWS and EKS. The following addons are available and work out of the box.","title":"EKS addons module"},{"location":"user-guides/eks-addons/#helm-charts","text":"All charts have been tested with Helm v3 and the terraform-provider-helm v1.0 which supports Helm v3. They can be easily customize with custom values. cluster-autoscaler : scale worker nodes based on workload. external-dns : sync ingress and service records in route53. cert-manager : automatically generate TLS certificates, supports ACME v2. nginx-ingress : processes Ingress object and acts as a HTTP/HTTPS proxy (compatible with cert-manager). metrics-server : enable metrics API and horizontal pod scaling (HPA). prometheus-operator : Monitoring / Alerting / Dashboards. fluentd-cloudwatch : forwards logs to AWS Cloudwatch. node-problem-detector : Forwards node problems to Kubernetes events flux : Continuous Delivery with Gitops workflow. sealed-secrets : Technology agnostic, store secrets on git. kong : API Gateway ingress controller. keycloak : Identity and access management","title":"Helm charts"},{"location":"user-guides/eks-addons/#kubernetes-manifests","text":"Kubernetes manifests are deployed with terraform-provider-kubectl cni-metrics-helper : Provides cloudwatch metrics for VPC CNI plugins.","title":"Kubernetes Manifests"},{"location":"user-guides/eks-addons/#operator","text":"Some project are transitioning to Operators . Istio is going to drop Helm support and is not compatible with Helm v3 so it has been removed and replaced with the Istio operator istio-operator : Service mesh for Kubernetes.","title":"Operator"},{"location":"user-guides/eks-addons/#iam-permissions","text":"Some addons require specific IAM permission. This can be done by either: IRSA: IAM role for service account which is the default and recommended way Addons that need IAM access have two variables: create_resources_irsa : default to true and uses IAM role for service account There is no specific config, everything is taken care of by the module.","title":"IAM permissions"},{"location":"user-guides/eks-addons/#customization","text":"All the configuration is done in eks-addons/terragrunt.hcl . i n clude { pa t h = \"${find_in_parent_folders()}\" } terraf orm { source = \"github.com/particuleio/terraform-kubernetes-addons.git//modules/aws?ref=main\" } depe n de n cy \"eks\" { co nf ig_pa t h = \"../eks\" mock_ou t pu ts = { clus ter _id = \"cluster-name\" clus ter _oidc_issuer_url = \"https://oidc.eks.eu-west-3.amazonaws.com/id/0000000000000000\" } } depe n de n cy \"vpc\" { co nf ig_pa t h = \"../vpc\" mock_ou t pu ts = { priva te _sub nets _cidr_blocks = [ \"10.0.0.0/16\" , \"192.168.0.0/24\" ] } } ge nerate \"provider\" { pa t h = \"provider.tf\" i f _exis ts = \"overwrite\" co ntents = << -E OF provider \"aws\" { regio n = \"${local.aws_region}\" } provider \"kubectl\" { hos t = da ta .aws_eks_clus ter .clus ter .e n dpoi nt clus ter _ca_cer t i f ica te = base 64 decode(da ta .aws_eks_clus ter .clus ter .cer t i f ica te _au t hori t y. 0. da ta ) t oke n = da ta .aws_eks_clus ter _au t h.clus ter . t oke n load_co nf ig_ f ile = false } provider \"kubernetes\" { hos t = da ta .aws_eks_clus ter .clus ter .e n dpoi nt clus ter _ca_cer t i f ica te = base 64 decode(da ta .aws_eks_clus ter .clus ter .cer t i f ica te _au t hori t y. 0. da ta ) t oke n = da ta .aws_eks_clus ter _au t h.clus ter . t oke n } provider \"helm\" { kuber netes { hos t = da ta .aws_eks_clus ter .clus ter .e n dpoi nt clus ter _ca_cer t i f ica te = base 64 decode(da ta .aws_eks_clus ter .clus ter .cer t i f ica te _au t hori t y. 0. da ta ) t oke n = da ta .aws_eks_clus ter _au t h.clus ter . t oke n } } da ta \"aws_eks_cluster\" \"cluster\" { na me = var.clus ter - na me } da ta \"aws_eks_cluster_auth\" \"cluster\" { na me = var.clus ter - na me } EOF } locals { aws_regio n = yamldecode( f ile( \"${find_in_parent_folders(\" regio n _values.yaml \")}\" )) [ \"aws_region\" ] cus t om_ ta gs = merge( yamldecode( f ile( \"${find_in_parent_folders(\" global_ ta gs.yaml \")}\" )) , yamldecode( f ile( \"${find_in_parent_folders(\" e n v_ ta gs.yaml \")}\" )) ) de fault _domai n _ na me = yamldecode( f ile( \"${find_in_parent_folders(\" global_values.yaml \")}\" )) [ \"default_domain_name\" ] de fault _domai n _su ff ix = \"${local.custom_tags[\" E n v \"]}.${local.custom_tags[\" Projec t \"]}.${local.default_domain_name}\" } i n pu ts = { clus ter - na me = depe n de n cy.eks.ou t pu ts .clus ter _id ta gs = merge( local.cus t om_ ta gs ) eks = { \"cluster_oidc_issuer_url\" = depe n de n cy.eks.ou t pu ts .clus ter _oidc_issuer_url } aws -e bs - csi - driver = { e na bled = true is_de fault _class = true } aws - f or - fluent - bi t = { e na bled = true } aws - load - bala n cer - co ntr oller = { e na bled = true } aws - n ode - ter mi nat io n - ha n dler = { e na bled = true } calico = { e na bled = true } cer t - ma na ger = { e na bled = true acme_email = \"cert@particule.io\" acme_h tt p 01 _e na bled = true acme_h tt p 01 _i n gress_class = \"nginx\" acme_d ns 01 _e na bled = true allowed_cidrs = depe n de n cy.vpc.ou t pu ts .priva te _sub nets _cidr_blocks experime ntal _csi_driver = true } clus ter - au t oscaler = { e na bled = true } c n i - me tr ics - helper = { e na bled = true } ex ternal - d ns = { ex ternal - d ns = { e na bled = true }, } i n gress - n gi n x = { e na bled = true use_ nl b_ip = true allowed_cidrs = depe n de n cy.vpc.ou t pu ts .priva te _sub nets _cidr_blocks } is t io - opera t or = { e na bled = true } karma = { e na bled = true ex tra _values = << -E XTRA_VALUES i n gress : e na bled : true pa t h : / a nn o tat io ns : kuber netes .io/i n gress.class : n gi n x cer t - ma na ger.io/clus ter - issuer : \"letsencrypt\" hos ts : - karma.$ { local.de fault _domai n _su ff ix } tls : - secre t Name : karma.$ { local.de fault _domai n _su ff ix } hos ts : - karma.$ { local.de fault _domai n _su ff ix } e n v : - na me : ALERTMANAGER_URI value : \"http://kube-prometheus-stack-alertmanager.monitoring.svc.cluster.local:9093\" - na me : ALERTMANAGER_PROXY value : \"true\" - na me : FILTERS_DEFAULT value : \"@state=active severity!=info severity!=none\" EXTRA_VALUES } keycloak = { e na bled = true } ko n g = { e na bled = true } kube - prome t heus - s ta ck = { e na bled = true allowed_cidrs = depe n de n cy.vpc.ou t pu ts .priva te _sub nets _cidr_blocks t ha n os_sidecar_e na bled = true t ha n os_bucke t _ f orce_des tr oy = true ex tra _values = << -E XTRA_VALUES gra fana : deployme nt S trate gy : t ype : Recrea te i n gress : e na bled : true a nn o tat io ns : kuber netes .io/i n gress.class : n gi n x cer t - ma na ger.io/clus ter - issuer : \"letsencrypt\" hos ts : - gra fana .$ { local.de fault _domai n _su ff ix } tls : - secre t Name : gra fana .$ { local.de fault _domai n _su ff ix } hos ts : - gra fana .$ { local.de fault _domai n _su ff ix } persis ten ce : e na bled : true s t orageClassName : ebs - sc accessModes : - ReadWri te O n ce size : 1 Gi prome t heus : prome t heusSpec : replicas : 1 re tent io n : 2 d re tent io n Size : \"6GB\" ruleSelec t orNilUsesHelmValues : false serviceMo n i t orSelec t orNilUsesHelmValues : false podMo n i t orSelec t orNilUsesHelmValues : false s t orageSpec : volumeClaimTempla te : spec : s t orageClassName : ebs - sc accessModes : [ \"ReadWriteOnce\" ] resources : reques ts : s t orage : 10 Gi EXTRA_VALUES } loki - s ta ck = { e na bled = true bucke t _ f orce_des tr oy = true } me tr ics - server = { e na bled = true allowed_cidrs = depe n de n cy.vpc.ou t pu ts .priva te _sub nets _cidr_blocks } n pd = { e na bled = true } sealed - secre ts = { e na bled = true } t ha n os = { e na bled = true bucke t _ f orce_des tr oy = true } }","title":"Customization"},{"location":"user-guides/eks-addons/#default-charts-values","text":"Some values are defined by default directly into the module. These can off course be overridden and or merged/replaced. You can find the defaults values in the upstream module . Eg. default values for cluster-autoscaler are in cluster-autoscaler.tf .","title":"Default charts values"},{"location":"user-guides/eks-addons/#overriding-helm-provider-values","text":"Helm provider have defaults values defined here : helm_de faults _de faults = { a t omic = false clea nu p_o n _ fa il = false depe n de n cy_upda te = false disable_crd_hooks = false disable_webhooks = false f orce_upda te = false recrea te _pods = false re n der_subchar t _ n o tes = true replace = false rese t _values = false reuse_values = false skip_crds = false t imeou t = 3600 veri f y = false wai t = true ex tra _values = \"\" } These can be overridden globally with the helm_defaults input variable or can be overridden per chart in terragrunt.hcl : helm_de faults = { replace = true veri f y = true t imeou t = 300 } clus ter _au t oscaler = { crea te _iam_resources_irsa = true iam_policy_override = \"\" versio n = \"v1.14.7\" char t _versio n = \"6.4.0\" e na bled = true de fault _ net work_policy = true clus ter _ na me = depe n de n cy.eks.ou t pu ts .clus ter _id t imeou t = 3600 <= here you ca n add a n y helm provider override }","title":"Overriding Helm provider values"},{"location":"user-guides/eks-addons/#overriding-charts-valuesyaml","text":"It is possible to add or override values per charts. Helm provider use the same merge logic as Helm so you can basically rewrite the whole values.yaml if needed. Each chart has a extra_values variable where you can specify custom values. flu x = { crea te _iam_resources_irsa = true versio n = \"1.18.0\" char t _versio n = \"1.2.0\" e na bled = false de fault _ net work_policy = true ex tra _values = <<EXTRA_VALUES gi t : url : \"ssh://git@gitlab.com/myrepo/gitops-${local.env}.git\" pollI nter val : \"2m\" rbac : crea te : false regis tr y : au t oma t io n I nter val : \"2m\" EXTRA_VALUES } There are some examples in the terragrunt.hcl file. Not all the variables available are present. If you want a full list of variable, you can find them in the upstream module . For example for cluster-autoscaler you can see the default here .","title":"Overriding charts values.yaml"},{"location":"user-guides/eks/","text":"EKS module \u00b6 Upstream configuration \u00b6 EKS module is also upstream and allow to deploy an EKS cluster which supports: managed node pools launch configuration node pools launch template node pools tEKS uses launch template by default and use one node pool per availability zone. You can use any inputs from the upstream module to configure the cluster in eks/terragrunt.hcl . i n clude { pa t h = \"${find_in_parent_folders()}\" } terraf orm { source = \"github.com/terraform-aws-modules/terraform-aws-eks?ref=master\" a fter _hook \"kubeconfig\" { comma n ds = [ \"apply\" ] execu te = [ \"bash\" , \"-c\" , \"terraform output --raw kubeconfig 2>/dev/null > ${get_terragrunt_dir()}/kubeconfig\" ] } a fter _hook \"kubeconfig-tg\" { comma n ds = [ \"apply\" ] execu te = [ \"bash\" , \"-c\" , \"terraform output --raw kubeconfig 2>/dev/null > kubeconfig\" ] } a fter _hook \"kube-system-label\" { comma n ds = [ \"apply\" ] execu te = [ \"bash\" , \"-c\" , \"kubectl --kubeconfig kubeconfig label ns kube-system name=kube-system --overwrite\" ] } a fter _hook \"undefault-gp2\" { comma n ds = [ \"apply\" ] execu te = [ \"bash\" , \"-c\" , \"kubectl --kubeconfig kubeconfig patch storageclass gp2 -p '{\\\"metadata\\\": {\\\"annotations\\\":{\\\"storageclass.kubernetes.io/is-default-class\\\":\\\"false\\\"}}}'\" ] } } locals { aws_regio n = yamldecode( f ile( \"${find_in_parent_folders(\" regio n _values.yaml \")}\" )) [ \"aws_region\" ] e n v = yamldecode( f ile( \"${find_in_parent_folders(\" e n v_ ta gs.yaml \")}\" )) [ \"Env\" ] pre f ix = yamldecode( f ile( \"${find_in_parent_folders(\" global_values.yaml \")}\" )) [ \"prefix\" ] na me = yamldecode( f ile( \"${find_in_parent_folders(\" clus ter _values.yaml \")}\" )) [ \"name\" ] cus t om_ ta gs = merge( yamldecode( f ile( \"${find_in_parent_folders(\" global_ ta gs.yaml \")}\" )) , yamldecode( f ile( \"${find_in_parent_folders(\" e n v_ ta gs.yaml \")}\" )) ) clus ter _ na me = \"${local.prefix}-${local.env}-${local.name}\" } depe n de n cy \"vpc\" { co nf ig_pa t h = \"../vpc\" mock_ou t pu ts = { vpc_id = \"vpc-00000000\" priva te _sub nets = [ \"subnet-00000000\" , \"subnet-00000001\" , \"subnet-00000002\" , ] } } ge nerate \"provider\" { pa t h = \"provider.tf\" i f _exis ts = \"overwrite\" co ntents = << -E OF provider \"aws\" { regio n = \"${local.aws_region}\" } provider \"kubernetes\" { hos t = da ta .aws_eks_clus ter .clus ter .e n dpoi nt clus ter _ca_cer t i f ica te = base 64 decode(da ta .aws_eks_clus ter .clus ter .cer t i f ica te _au t hori t y. 0. da ta ) t oke n = da ta .aws_eks_clus ter _au t h.clus ter . t oke n } da ta \"aws_eks_cluster\" \"cluster\" { na me = aws_eks_clus ter . t his [ 0 ] .id } da ta \"aws_eks_cluster_auth\" \"cluster\" { na me = aws_eks_clus ter . t his [ 0 ] .id } EOF } i n pu ts = { aws = { \"region\" = local.aws_regio n } ta gs = merge( local.cus t om_ ta gs ) clus ter _ na me = local.clus ter _ na me sub nets = depe n de n cy.vpc.ou t pu ts .priva te _sub nets vpc_id = depe n de n cy.vpc.ou t pu ts .vpc_id wri te _kubeco nf ig = true e na ble_irsa = true kubeco nf ig_aws_au t he nt ica t or_comma n d = \"aws\" kubeco nf ig_aws_au t he nt ica t or_comma n d_args = [ \"eks\" , \"get-token\" , \"--cluster-name\" , local.clus ter _ na me ] kubeco nf ig_aws_au t he nt ica t or_addi t io nal _args = [] clus ter _versio n = \"1.19\" clus ter _e na bled_log_ t ypes = [ \"api\" , \"audit\" , \"authenticator\" , \"controllerManager\" , \"scheduler\" ] n ode_groups = { \"default-${local.aws_region}\" = { crea te _lau n ch_ te mpla te = true desired_capaci t y = 3 max_capaci t y = 5 mi n _capaci t y = 1 i nstan ce_ t ypes = [ \"m5a.large\" ] disk_size = 50 k 8 s_labels = { pool = \"default\" } capaci t y_ t ype = \"ON_DEMAND\" } \"dedicated-${local.aws_region}\" = { crea te _lau n ch_ te mpla te = true desired_capaci t y = 3 max_capaci t y = 5 mi n _capaci t y = 3 i nstan ce_ t ypes = [ \"m5a.large\" ] disk_size = 50 kubele t _ex tra _args = \"--register-with-taints=dedicated=spot:NoSchedule\" k 8 s_labels = { pool = \"dedicated\" } capaci t y_ t ype = \"SPOT\" } } }","title":"EKS"},{"location":"user-guides/eks/#eks-module","text":"","title":"EKS module"},{"location":"user-guides/eks/#upstream-configuration","text":"EKS module is also upstream and allow to deploy an EKS cluster which supports: managed node pools launch configuration node pools launch template node pools tEKS uses launch template by default and use one node pool per availability zone. You can use any inputs from the upstream module to configure the cluster in eks/terragrunt.hcl . i n clude { pa t h = \"${find_in_parent_folders()}\" } terraf orm { source = \"github.com/terraform-aws-modules/terraform-aws-eks?ref=master\" a fter _hook \"kubeconfig\" { comma n ds = [ \"apply\" ] execu te = [ \"bash\" , \"-c\" , \"terraform output --raw kubeconfig 2>/dev/null > ${get_terragrunt_dir()}/kubeconfig\" ] } a fter _hook \"kubeconfig-tg\" { comma n ds = [ \"apply\" ] execu te = [ \"bash\" , \"-c\" , \"terraform output --raw kubeconfig 2>/dev/null > kubeconfig\" ] } a fter _hook \"kube-system-label\" { comma n ds = [ \"apply\" ] execu te = [ \"bash\" , \"-c\" , \"kubectl --kubeconfig kubeconfig label ns kube-system name=kube-system --overwrite\" ] } a fter _hook \"undefault-gp2\" { comma n ds = [ \"apply\" ] execu te = [ \"bash\" , \"-c\" , \"kubectl --kubeconfig kubeconfig patch storageclass gp2 -p '{\\\"metadata\\\": {\\\"annotations\\\":{\\\"storageclass.kubernetes.io/is-default-class\\\":\\\"false\\\"}}}'\" ] } } locals { aws_regio n = yamldecode( f ile( \"${find_in_parent_folders(\" regio n _values.yaml \")}\" )) [ \"aws_region\" ] e n v = yamldecode( f ile( \"${find_in_parent_folders(\" e n v_ ta gs.yaml \")}\" )) [ \"Env\" ] pre f ix = yamldecode( f ile( \"${find_in_parent_folders(\" global_values.yaml \")}\" )) [ \"prefix\" ] na me = yamldecode( f ile( \"${find_in_parent_folders(\" clus ter _values.yaml \")}\" )) [ \"name\" ] cus t om_ ta gs = merge( yamldecode( f ile( \"${find_in_parent_folders(\" global_ ta gs.yaml \")}\" )) , yamldecode( f ile( \"${find_in_parent_folders(\" e n v_ ta gs.yaml \")}\" )) ) clus ter _ na me = \"${local.prefix}-${local.env}-${local.name}\" } depe n de n cy \"vpc\" { co nf ig_pa t h = \"../vpc\" mock_ou t pu ts = { vpc_id = \"vpc-00000000\" priva te _sub nets = [ \"subnet-00000000\" , \"subnet-00000001\" , \"subnet-00000002\" , ] } } ge nerate \"provider\" { pa t h = \"provider.tf\" i f _exis ts = \"overwrite\" co ntents = << -E OF provider \"aws\" { regio n = \"${local.aws_region}\" } provider \"kubernetes\" { hos t = da ta .aws_eks_clus ter .clus ter .e n dpoi nt clus ter _ca_cer t i f ica te = base 64 decode(da ta .aws_eks_clus ter .clus ter .cer t i f ica te _au t hori t y. 0. da ta ) t oke n = da ta .aws_eks_clus ter _au t h.clus ter . t oke n } da ta \"aws_eks_cluster\" \"cluster\" { na me = aws_eks_clus ter . t his [ 0 ] .id } da ta \"aws_eks_cluster_auth\" \"cluster\" { na me = aws_eks_clus ter . t his [ 0 ] .id } EOF } i n pu ts = { aws = { \"region\" = local.aws_regio n } ta gs = merge( local.cus t om_ ta gs ) clus ter _ na me = local.clus ter _ na me sub nets = depe n de n cy.vpc.ou t pu ts .priva te _sub nets vpc_id = depe n de n cy.vpc.ou t pu ts .vpc_id wri te _kubeco nf ig = true e na ble_irsa = true kubeco nf ig_aws_au t he nt ica t or_comma n d = \"aws\" kubeco nf ig_aws_au t he nt ica t or_comma n d_args = [ \"eks\" , \"get-token\" , \"--cluster-name\" , local.clus ter _ na me ] kubeco nf ig_aws_au t he nt ica t or_addi t io nal _args = [] clus ter _versio n = \"1.19\" clus ter _e na bled_log_ t ypes = [ \"api\" , \"audit\" , \"authenticator\" , \"controllerManager\" , \"scheduler\" ] n ode_groups = { \"default-${local.aws_region}\" = { crea te _lau n ch_ te mpla te = true desired_capaci t y = 3 max_capaci t y = 5 mi n _capaci t y = 1 i nstan ce_ t ypes = [ \"m5a.large\" ] disk_size = 50 k 8 s_labels = { pool = \"default\" } capaci t y_ t ype = \"ON_DEMAND\" } \"dedicated-${local.aws_region}\" = { crea te _lau n ch_ te mpla te = true desired_capaci t y = 3 max_capaci t y = 5 mi n _capaci t y = 3 i nstan ce_ t ypes = [ \"m5a.large\" ] disk_size = 50 kubele t _ex tra _args = \"--register-with-taints=dedicated=spot:NoSchedule\" k 8 s_labels = { pool = \"dedicated\" } capaci t y_ t ype = \"SPOT\" } } }","title":"Upstream configuration"},{"location":"user-guides/getting-started/","text":"Getting started \u00b6 Tooling requirements \u00b6 The necessary tools are in requirements.yaml you can install them any way you want, make sure they are available in your $PATH. The following dependencies are required on the deployer host: Terraform Terragrunt kubectl helm aws-iam-authenticator AWS requirements \u00b6 At least one AWS account awscli configured ( see installation instructions ) to access your AWS account. A route53 hosted zone if you plan to use external-dns or cert-manager but it is not a hard requirement. Getting the template repository \u00b6 You can either clone the repo locally or generate/fork a template from github. git clone https://github.com/particuleio/teks.git The terraform directory structure is the following: . \u2514\u2500\u2500 live \u251c\u2500\u2500 backend \u2502 \u251c\u2500\u2500 backend.tf \u2502 \u251c\u2500\u2500 providers.tf \u2502 \u2514\u2500\u2500 state.tf \u251c\u2500\u2500 demo \u2502 \u251c\u2500\u2500 env_tags.yaml \u2502 \u2514\u2500\u2500 eu-west-3 \u2502 \u251c\u2500\u2500 clusters \u2502 \u2502 \u2514\u2500\u2500 full \u2502 \u2502 \u251c\u2500\u2500 eks \u2502 \u2502 \u2502 \u251c\u2500\u2500 aws-provider.tf -> ../../../../../shared/aws-provider.tf \u2502 \u2502 \u2502 \u251c\u2500\u2500 backend.tf -> ../../../../../backend/backend.tf \u2502 \u2502 \u2502 \u251c\u2500\u2500 data.tf \u2502 \u2502 \u2502 \u251c\u2500\u2500 locals.tf -> ../../../../../shared/locals.tf \u2502 \u2502 \u2502 \u2514\u2500\u2500 main.tf \u2502 \u2502 \u251c\u2500\u2500 eks-addons \u2502 \u2502 \u2502 \u251c\u2500\u2500 aws-provider.tf -> ../../../../../shared/aws-provider.tf \u2502 \u2502 \u2502 \u251c\u2500\u2500 backend.tf -> ../../../../../backend/backend.tf \u2502 \u2502 \u2502 \u251c\u2500\u2500 data.tf \u2502 \u2502 \u2502 \u251c\u2500\u2500 locals.tf -> ../../../../../shared/locals.tf \u2502 \u2502 \u2502 \u251c\u2500\u2500 main.tf \u2502 \u2502 \u2502 \u2514\u2500\u2500 versions.tf \u2502 \u2502 \u2514\u2500\u2500 vpc \u2502 \u2502 \u251c\u2500\u2500 aws-provider.tf -> ../../../../../shared/aws-provider.tf \u2502 \u2502 \u251c\u2500\u2500 backend.tf -> ../../../../../backend/backend.tf \u2502 \u2502 \u251c\u2500\u2500 locals.tf -> ../../../../../shared/locals.tf \u2502 \u2502 \u2514\u2500\u2500 main.tf \u2502 \u2514\u2500\u2500 region_values.yaml \u251c\u2500\u2500 global_tags.yaml \u251c\u2500\u2500 global_values.yaml \u2514\u2500\u2500 shared \u251c\u2500\u2500 aws-provider.tf \u2514\u2500\u2500 locals.tf Each cluster in inside the terraform/live folder and then modules are grouped by AWS region. Start a new cluster \u00b6 Create a new cluster beside demo : cp -ar demo mycluster Configuring the remote state \u00b6 Configuration of the remote state is based on the value of the global_values.yaml file in the terraform and the terragrunt directories based on the installation method you used. Both files are following the same structure. --- aws_account_id : 161285725140 default_domain_name : clusterfrak-dynamics.io prefix : pio project : teks tf_state_bucket_region : eu-west-3 Adapt these values to match your configuration ( prefix , 'project'). Based on the configuration, both methods will create the following resources: S3 bucket named {prefix}-{project}-{tf|tg}-state : store the state DynamoDB table named {prefix}-{project}-{tf|tg}-state-lock : prevent concurrent use The resource names will include information based on the configuration method used. Using terraform will create resources with tf in their name, and tg when using terragrunt . Using the current values, the resources created to use terraform will be: S3: pio-teks-tf-state DynamoDB: pio-state-tf-state-lock Remote state for Terraform \u00b6 If you plan on using terraform to setup teks , you need to create your remote backend using cloudposse/terraform-aws-tfstate-backend configured in terraform/live/backend/state.tf . In order to configure the S3 backend for terraform, configure your global_values.yaml then go in the terraform/live/backend directory. terraform init init the terraform module. terraform apply -auto-approve to create the S3 backend. terraform init -force-copy will copy the local backend to the S3 backend. The terraform-aws-tfstate-backend module will create or update the terraform/live/backend/backend.tf file, which is symlinked to the child modules ( vpc , eks , eks-addons ). terraf orm { required_versio n = \">= 0.12.2\" backe n d \"s3\" { regio n = \"eu-west-1\" bucke t = \"pio-teks-tf-state-lock\" key = \"terraform.tfstate\" dy na modb_ ta ble = \"pio-teks-tf-state-store-lock\" pro f ile = \"\" role_ar n = \"\" e n cryp t = \"true\" } } Further documentation regarding the remote backend configuration can be found at terraform-aws-tfstate-backend#create . Remote state for Terragrunt \u00b6 terragrunt/live/demo/terragrunt.hcl is the parent terragrunt file use to configure remote state. The configuration is done automatically based on the terragrunt/live/global_values.yaml file. The values here will generate automatically the parent terragrunt file. remo te _s tate { backe n d = \"s3\" co nf ig = { bucke t = \"${yamldecode(file(\" global_values.yaml \"))[\" pre f ix \"]}-${yamldecode(file(\" global_values.yaml \"))[\" projec t \"]}-tg-state-store\" key = \"${path_relative_to_include()}/terraform.tfstate\" regio n = \"${yamldecode(file(\" global_values.yaml \"))[\" tf _s tate _bucke t _regio n \"]}\" e n cryp t = true dy na modb_ ta ble = \"${yamldecode(file(\" global_values.yaml \"))[\" pre f ix \"]}-${yamldecode(file(\" global_values.yaml \"))[\" projec t \"]}-tg-state-lock\" } ge nerate = { pa t h = \"backend.tf\" i f _exis ts = \"overwrite_terragrunt\" } } # Use t his t o imperso nate a role , use ful f or EKS whe n you wa nt a role t o be # t he \"root\" use a n d n o t a perso nal AWS accou nt # iam_role = \"arn:aws:iam::${yamldecode(file(\" global_values.yaml \"))[\" aws_accou nt _id \"]}:role/administrator\" You can either customize the values or edit directly the terragrunt.hcl file. Running Terragrunt command \u00b6 Terragrunt command are run inside their respective folder, for example, to run the vpc module: cd vpc terragrunt apply","title":"Getting started"},{"location":"user-guides/getting-started/#getting-started","text":"","title":"Getting started"},{"location":"user-guides/getting-started/#tooling-requirements","text":"The necessary tools are in requirements.yaml you can install them any way you want, make sure they are available in your $PATH. The following dependencies are required on the deployer host: Terraform Terragrunt kubectl helm aws-iam-authenticator","title":"Tooling requirements"},{"location":"user-guides/getting-started/#aws-requirements","text":"At least one AWS account awscli configured ( see installation instructions ) to access your AWS account. A route53 hosted zone if you plan to use external-dns or cert-manager but it is not a hard requirement.","title":"AWS requirements"},{"location":"user-guides/getting-started/#getting-the-template-repository","text":"You can either clone the repo locally or generate/fork a template from github. git clone https://github.com/particuleio/teks.git The terraform directory structure is the following: . \u2514\u2500\u2500 live \u251c\u2500\u2500 backend \u2502 \u251c\u2500\u2500 backend.tf \u2502 \u251c\u2500\u2500 providers.tf \u2502 \u2514\u2500\u2500 state.tf \u251c\u2500\u2500 demo \u2502 \u251c\u2500\u2500 env_tags.yaml \u2502 \u2514\u2500\u2500 eu-west-3 \u2502 \u251c\u2500\u2500 clusters \u2502 \u2502 \u2514\u2500\u2500 full \u2502 \u2502 \u251c\u2500\u2500 eks \u2502 \u2502 \u2502 \u251c\u2500\u2500 aws-provider.tf -> ../../../../../shared/aws-provider.tf \u2502 \u2502 \u2502 \u251c\u2500\u2500 backend.tf -> ../../../../../backend/backend.tf \u2502 \u2502 \u2502 \u251c\u2500\u2500 data.tf \u2502 \u2502 \u2502 \u251c\u2500\u2500 locals.tf -> ../../../../../shared/locals.tf \u2502 \u2502 \u2502 \u2514\u2500\u2500 main.tf \u2502 \u2502 \u251c\u2500\u2500 eks-addons \u2502 \u2502 \u2502 \u251c\u2500\u2500 aws-provider.tf -> ../../../../../shared/aws-provider.tf \u2502 \u2502 \u2502 \u251c\u2500\u2500 backend.tf -> ../../../../../backend/backend.tf \u2502 \u2502 \u2502 \u251c\u2500\u2500 data.tf \u2502 \u2502 \u2502 \u251c\u2500\u2500 locals.tf -> ../../../../../shared/locals.tf \u2502 \u2502 \u2502 \u251c\u2500\u2500 main.tf \u2502 \u2502 \u2502 \u2514\u2500\u2500 versions.tf \u2502 \u2502 \u2514\u2500\u2500 vpc \u2502 \u2502 \u251c\u2500\u2500 aws-provider.tf -> ../../../../../shared/aws-provider.tf \u2502 \u2502 \u251c\u2500\u2500 backend.tf -> ../../../../../backend/backend.tf \u2502 \u2502 \u251c\u2500\u2500 locals.tf -> ../../../../../shared/locals.tf \u2502 \u2502 \u2514\u2500\u2500 main.tf \u2502 \u2514\u2500\u2500 region_values.yaml \u251c\u2500\u2500 global_tags.yaml \u251c\u2500\u2500 global_values.yaml \u2514\u2500\u2500 shared \u251c\u2500\u2500 aws-provider.tf \u2514\u2500\u2500 locals.tf Each cluster in inside the terraform/live folder and then modules are grouped by AWS region.","title":"Getting the template repository"},{"location":"user-guides/getting-started/#start-a-new-cluster","text":"Create a new cluster beside demo : cp -ar demo mycluster","title":"Start a new cluster"},{"location":"user-guides/getting-started/#configuring-the-remote-state","text":"Configuration of the remote state is based on the value of the global_values.yaml file in the terraform and the terragrunt directories based on the installation method you used. Both files are following the same structure. --- aws_account_id : 161285725140 default_domain_name : clusterfrak-dynamics.io prefix : pio project : teks tf_state_bucket_region : eu-west-3 Adapt these values to match your configuration ( prefix , 'project'). Based on the configuration, both methods will create the following resources: S3 bucket named {prefix}-{project}-{tf|tg}-state : store the state DynamoDB table named {prefix}-{project}-{tf|tg}-state-lock : prevent concurrent use The resource names will include information based on the configuration method used. Using terraform will create resources with tf in their name, and tg when using terragrunt . Using the current values, the resources created to use terraform will be: S3: pio-teks-tf-state DynamoDB: pio-state-tf-state-lock","title":"Configuring the remote state"},{"location":"user-guides/getting-started/#remote-state-for-terraform","text":"If you plan on using terraform to setup teks , you need to create your remote backend using cloudposse/terraform-aws-tfstate-backend configured in terraform/live/backend/state.tf . In order to configure the S3 backend for terraform, configure your global_values.yaml then go in the terraform/live/backend directory. terraform init init the terraform module. terraform apply -auto-approve to create the S3 backend. terraform init -force-copy will copy the local backend to the S3 backend. The terraform-aws-tfstate-backend module will create or update the terraform/live/backend/backend.tf file, which is symlinked to the child modules ( vpc , eks , eks-addons ). terraf orm { required_versio n = \">= 0.12.2\" backe n d \"s3\" { regio n = \"eu-west-1\" bucke t = \"pio-teks-tf-state-lock\" key = \"terraform.tfstate\" dy na modb_ ta ble = \"pio-teks-tf-state-store-lock\" pro f ile = \"\" role_ar n = \"\" e n cryp t = \"true\" } } Further documentation regarding the remote backend configuration can be found at terraform-aws-tfstate-backend#create .","title":"Remote state for Terraform"},{"location":"user-guides/getting-started/#remote-state-for-terragrunt","text":"terragrunt/live/demo/terragrunt.hcl is the parent terragrunt file use to configure remote state. The configuration is done automatically based on the terragrunt/live/global_values.yaml file. The values here will generate automatically the parent terragrunt file. remo te _s tate { backe n d = \"s3\" co nf ig = { bucke t = \"${yamldecode(file(\" global_values.yaml \"))[\" pre f ix \"]}-${yamldecode(file(\" global_values.yaml \"))[\" projec t \"]}-tg-state-store\" key = \"${path_relative_to_include()}/terraform.tfstate\" regio n = \"${yamldecode(file(\" global_values.yaml \"))[\" tf _s tate _bucke t _regio n \"]}\" e n cryp t = true dy na modb_ ta ble = \"${yamldecode(file(\" global_values.yaml \"))[\" pre f ix \"]}-${yamldecode(file(\" global_values.yaml \"))[\" projec t \"]}-tg-state-lock\" } ge nerate = { pa t h = \"backend.tf\" i f _exis ts = \"overwrite_terragrunt\" } } # Use t his t o imperso nate a role , use ful f or EKS whe n you wa nt a role t o be # t he \"root\" use a n d n o t a perso nal AWS accou nt # iam_role = \"arn:aws:iam::${yamldecode(file(\" global_values.yaml \"))[\" aws_accou nt _id \"]}:role/administrator\" You can either customize the values or edit directly the terragrunt.hcl file.","title":"Remote state for Terragrunt"},{"location":"user-guides/getting-started/#running-terragrunt-command","text":"Terragrunt command are run inside their respective folder, for example, to run the vpc module: cd vpc terragrunt apply","title":"Running Terragrunt command"},{"location":"user-guides/vpc/","text":"VPC module \u00b6 The vpc module is the one from upstream . To customize it. Modify the vpc/terragrunt.hcl file. You can use any inputs available in the upstream module. i n clude { pa t h = \"${find_in_parent_folders()}\" } terraf orm { source = \"github.com/terraform-aws-modules/terraform-aws-vpc?ref=v3.0.0\" } locals { aws_regio n = yamldecode( f ile( \"${find_in_parent_folders(\" regio n _values.yaml \")}\" )) [ \"aws_region\" ] e n v = yamldecode( f ile( \"${find_in_parent_folders(\" e n v_ ta gs.yaml \")}\" )) [ \"Env\" ] pre f ix = yamldecode( f ile( \"${find_in_parent_folders(\" global_values.yaml \")}\" )) [ \"prefix\" ] na me = yamldecode( f ile( \"${find_in_parent_folders(\" clus ter _values.yaml \")}\" )) [ \"name\" ] cus t om_ ta gs = merge( yamldecode( f ile( \"${find_in_parent_folders(\" global_ ta gs.yaml \")}\" )) , yamldecode( f ile( \"${find_in_parent_folders(\" e n v_ ta gs.yaml \")}\" )) ) clus ter _ na me = \"${local.prefix}-${local.env}-${local.name}\" } ge nerate \"provider\" { pa t h = \"provider.tf\" i f _exis ts = \"overwrite\" co ntents = << -E OF provider \"aws\" { regio n = \"${local.aws_region}\" } EOF } i n pu ts = { ta gs = merge( { \"kubernetes.io/cluster/${local.cluster_name}\" = \"shared\" }, local.cus t om_ ta gs ) na me = \"vpc-eks-${local.env}\" cidr = \"10.0.0.0/16\" azs = [ \"${local.aws_region}a\" , \"${local.aws_region}b\" , \"${local.aws_region}c\" ] priva te _sub nets = [ \"10.0.1.0/24\" , \"10.0.2.0/24\" , \"10.0.3.0/24\" ] public_sub nets = [ \"10.0.101.0/24\" , \"10.0.102.0/24\" , \"10.0.103.0/24\" ] e na ble_ipv 6 = true assig n _ipv 6 _address_o n _crea t io n = true public_sub net _ipv 6 _pre f ixes = [ 0 , 1 , 2 ] priva te _sub net _ipv 6 _pre f ixes = [ 3 , 4 , 5 ] e na ble_ nat _ga te way = true si n gle_ nat _ga te way = true e na ble_d ns _hos tna mes = true e na ble_d ns _suppor t = true public_sub net _ ta gs = { \"kubernetes.io/cluster/${local.cluster_name}\" = \"shared\" \"kubernetes.io/role/elb\" = \"1\" } priva te _sub net _ ta gs = { \"kubernetes.io/cluster/${local.cluster_name}\" = \"shared\" \"kubernetes.io/role/internal-elb\" = \"1\" } }","title":"VPC"},{"location":"user-guides/vpc/#vpc-module","text":"The vpc module is the one from upstream . To customize it. Modify the vpc/terragrunt.hcl file. You can use any inputs available in the upstream module. i n clude { pa t h = \"${find_in_parent_folders()}\" } terraf orm { source = \"github.com/terraform-aws-modules/terraform-aws-vpc?ref=v3.0.0\" } locals { aws_regio n = yamldecode( f ile( \"${find_in_parent_folders(\" regio n _values.yaml \")}\" )) [ \"aws_region\" ] e n v = yamldecode( f ile( \"${find_in_parent_folders(\" e n v_ ta gs.yaml \")}\" )) [ \"Env\" ] pre f ix = yamldecode( f ile( \"${find_in_parent_folders(\" global_values.yaml \")}\" )) [ \"prefix\" ] na me = yamldecode( f ile( \"${find_in_parent_folders(\" clus ter _values.yaml \")}\" )) [ \"name\" ] cus t om_ ta gs = merge( yamldecode( f ile( \"${find_in_parent_folders(\" global_ ta gs.yaml \")}\" )) , yamldecode( f ile( \"${find_in_parent_folders(\" e n v_ ta gs.yaml \")}\" )) ) clus ter _ na me = \"${local.prefix}-${local.env}-${local.name}\" } ge nerate \"provider\" { pa t h = \"provider.tf\" i f _exis ts = \"overwrite\" co ntents = << -E OF provider \"aws\" { regio n = \"${local.aws_region}\" } EOF } i n pu ts = { ta gs = merge( { \"kubernetes.io/cluster/${local.cluster_name}\" = \"shared\" }, local.cus t om_ ta gs ) na me = \"vpc-eks-${local.env}\" cidr = \"10.0.0.0/16\" azs = [ \"${local.aws_region}a\" , \"${local.aws_region}b\" , \"${local.aws_region}c\" ] priva te _sub nets = [ \"10.0.1.0/24\" , \"10.0.2.0/24\" , \"10.0.3.0/24\" ] public_sub nets = [ \"10.0.101.0/24\" , \"10.0.102.0/24\" , \"10.0.103.0/24\" ] e na ble_ipv 6 = true assig n _ipv 6 _address_o n _crea t io n = true public_sub net _ipv 6 _pre f ixes = [ 0 , 1 , 2 ] priva te _sub net _ipv 6 _pre f ixes = [ 3 , 4 , 5 ] e na ble_ nat _ga te way = true si n gle_ nat _ga te way = true e na ble_d ns _hos tna mes = true e na ble_d ns _suppor t = true public_sub net _ ta gs = { \"kubernetes.io/cluster/${local.cluster_name}\" = \"shared\" \"kubernetes.io/role/elb\" = \"1\" } priva te _sub net _ ta gs = { \"kubernetes.io/cluster/${local.cluster_name}\" = \"shared\" \"kubernetes.io/role/internal-elb\" = \"1\" } }","title":"VPC module"}]}